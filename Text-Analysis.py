# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fit_KCL3s8P9-VCMFxAGz9v4dO9C-_7p
"""

# Colab-ready single cell: combined, fixed, and using default NLTK stopwords only.

# -----------------------------
# Install non-NLTK dependencies
# -----------------------------
!pip install -q pandas requests beautifulsoup4 textstat openpyxl

# Ensure nltk is installed/upgraded (do NOT uninstall in-session)
!pip install -q --upgrade nltk

# -----------------------------
# Imports and robust NLTK setup
# -----------------------------
import importlib
import nltk
import sys
import os
import time
import re
import csv
import requests
from bs4 import BeautifulSoup
import textstat
import pandas as pd
from typing import Tuple, List, Set

# Reload nltk to make sure it's in a consistent state if it was already imported earlier
importlib.reload(nltk)

# Prepare a fallback download directory (Colab-friendly)
NLTK_FALLBACK_DIR = "/root/nltk_data"
if NLTK_FALLBACK_DIR not in nltk.data.path:
    nltk.data.path.append(NLTK_FALLBACK_DIR)
    os.makedirs(NLTK_FALLBACK_DIR, exist_ok=True)

def ensure_nltk_resource(resource_path: str, download_name: str) -> bool:
    try:
        nltk.data.find(resource_path)
        return True
    except LookupError:
        try:
            print(f"Downloading NLTK resource '{download_name}' ...")
            nltk.download(download_name, quiet=False)
            nltk.data.find(resource_path)
            return True
        except Exception as e1:
            print(f"Normal nltk.download failed for {download_name}: {e1}")
            try:
                print(f"Retrying download to {NLTK_FALLBACK_DIR} ...")
                nltk.download(download_name, download_dir=NLTK_FALLBACK_DIR, quiet=False)
                if NLTK_FALLBACK_DIR not in nltk.data.path:
                    nltk.data.path.append(NLTK_FALLBACK_DIR)
                nltk.data.find(resource_path)
                return True
            except Exception as e2:
                print(f"Fallback download also failed for {download_name}: {e2}")
                return False

ok_punkt = ensure_nltk_resource("tokenizers/punkt", "punkt")
ok_stop = ensure_nltk_resource("corpora/stopwords", "stopwords")
ok_punkt_tab = ensure_nltk_resource("tokenizers/punkt_tab", "punkt_tab") # Add this line

if not (ok_punkt and ok_stop and ok_punkt_tab): # Update this condition
    raise RuntimeError("Failed to obtain required NLTK resources: 'punkt', 'punkt_tab' and/or 'stopwords'. "
                       "Try restarting runtime or checking network/firewall settings.")

# Now import the specific NLTK pieces you need
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords as nltk_stopwords

print("NLTK resources ready: punkt=", ok_punkt, " stopwords=", ok_stop, " punkt_tab=", ok_punkt_tab)

# -----------------------------
# CONFIG
# -----------------------------
INPUT_XLSX = "Input.xlsx"   # change if needed
OUTPUT_XLSX = "Filled_Output.xlsx"
CLEANED_CSV = "cleaned_texts.csv"
output = OUTPUT_XLSX
cleantext = CLEANED_CSV
if os.path.exists(output):
    os.remove(output)
    print(f"Removed existing {output}")
if os.path.exists(cleantext):
    os.remove(cleantext)
    print(f"Removed existing {cleantext}")

# if you have MasterDictionary files, keep the paths; they're optional
POS_DICT = "MasterDictionary/positive-words.txt"
NEG_DICT = "MasterDictionary/negative-words.txt"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0"
}

# -----------------------------
# HELPERS: LOAD STOPWORDS & DICTS
# -----------------------------
def load_stopwords_default() -> Set[str]:
    """
    Use ONLY the default NLTK English stopwords (as requested).
    """
    try:
        return set(nltk_stopwords.words('english'))
    except Exception as e:
        print(f"Warning: couldn't load NLTK stopwords: {e}. Returning empty set.")
        return set()

def load_wordlist(path: str) -> Set[str]:
    words = set()
    if not os.path.exists(path):
        print(f"Warning: {path} not found. Returning empty set.")
        return words
    try:
        with open(path, "r", encoding="ISO-8859-1") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith(";"):
                    continue
                words.add(line.lower())
    except Exception as e:
        print(f"Warning reading {path}: {e}")
    return words

# Load only NLTK default stopwords
stopwords = load_stopwords_default()
positive_words = load_wordlist(POS_DICT)
negative_words = load_wordlist(NEG_DICT)

# -----------------------------
# CLEANING / EXTRACTION
# -----------------------------
def fetch_html(url: str, timeout: int = 12) -> str:
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout)
        r.raise_for_status()
        if not r.encoding:
            r.encoding = 'utf-8'
        return r.text
    except Exception as e:
        print(f"Failed to fetch {url}: {e}")
        return ""

def extract_visible_text(html: str) -> str:
    if not html:
        return ""
    soup = BeautifulSoup(html, "html.parser")

    paragraphs = soup.find_all("p")
    if not paragraphs:
        article = soup.find("article")
        if article:
            paragraphs = article.find_all("p")

    if paragraphs:
        text = " ".join([p.get_text(" ", strip=True) for p in paragraphs])
    else:
        text = soup.get_text(" ", strip=True)

    return text.strip()

def clean_text_from_url(url: str) -> Tuple[List[str], str]:
    html = fetch_html(url)
    if not html:
        return [], ""

    raw_text = extract_visible_text(html)
    if not raw_text:
        return [], ""

    raw_for_sent = re.sub(r"\s+", " ", raw_text).strip()  # keep case for pronoun find, but later lower where needed

    # Lowercase for token cleaning
    raw_lower = raw_for_sent.lower()

    # letters only
    letters_only = re.sub(r"[^a-z\s]", " ", raw_lower)
    letters_only = re.sub(r"\s+", " ", letters_only).strip()

    if not letters_only:
        return [], raw_for_sent

    tokens = letters_only.split()

    # remove stopwords (ONLY DEFAULT NOW)
    cleaned_tokens = [t for t in tokens if t not in stopwords]

    return cleaned_tokens, raw_for_sent

# -----------------------------
# METRICS
# -----------------------------
def calculate_metrics(cleaned_words: List[str], raw_text_for_sent: str) -> List[float]:
    words = cleaned_words
    word_count = len(words)

    sentences = sent_tokenize(raw_text_for_sent) if raw_text_for_sent else []
    sentence_count = len(sentences) if len(sentences) > 0 else 1

    positive = sum(1 for w in words if w in positive_words)
    negative = sum(1 for w in words if w in negative_words)

    denom = positive + negative if (positive + negative) != 0 else 1e-6
    polarity = (positive - negative) / denom

    subjectivity = (positive + negative) / (word_count if word_count > 0 else 1e-6)

    avg_sentence_length = word_count / (sentence_count + 1e-6)

    complex_words = [w for w in words if textstat.syllable_count(w) > 2]
    complex_word_count = len(complex_words)
    percentage_complex_words = (complex_word_count / (word_count if word_count > 0 else 1e-6)) * 100

    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    total_syllables = sum(textstat.syllable_count(w) for w in words) if word_count > 0 else 0
    syllable_per_word = total_syllables / (word_count if word_count > 0 else 1e-6)

    pronouns = re.findall(r"\b(i|we|my|ours|us|me|mine|our)\b", raw_text_for_sent, flags=re.I)
    personal_pronouns = len(pronouns)

    avg_word_length = (sum(len(w) for w in words) / (word_count if word_count > 0 else 1e-6))

    return [
        positive, negative, polarity, subjectivity, avg_sentence_length,
        percentage_complex_words, fog_index, avg_sentence_length,
        complex_word_count, word_count, syllable_per_word,
        personal_pronouns, avg_word_length
    ]

# -----------------------------
# MAIN PROCESS
# -----------------------------
def main():
    if not os.path.exists(INPUT_XLSX):
        print(f"Input file '{INPUT_XLSX}' not found. Create or upload it to the Colab workspace and re-run.")
        return

    df = pd.read_excel(INPUT_XLSX)

    if "URL" not in df.columns:
        print("Error: 'URL' column missing. Make sure your input Excel has a column named 'URL'.")
        return

    all_cleaned_texts = []
    results = []

    total = len(df)
    for i, row in df.iterrows():
        url = str(row["URL"])
        print(f"[{i+1}/{total}] Processing: {url}")

        cleaned_words, raw_text = clean_text_from_url(url)
        all_cleaned_texts.append(cleaned_words)

        scores = calculate_metrics(cleaned_words, raw_text)
        results.append(scores)

        # polite pause to avoid hammering sites
        time.sleep(1)

    metric_names = [
        "POSITIVE SCORE", "NEGATIVE SCORE", "POLARITY SCORE", "SUBJECTIVITY SCORE",
        "AVG SENTENCE LENGTH", "PERCENTAGE OF COMPLEX WORDS", "FOG INDEX",
        "AVG NUMBER OF WORDS PER SENTENCE", "COMPLEX WORD COUNT", "WORD COUNT",
        "SYLLABLE PER WORD", "PERSONAL PRONOUNS", "AVG WORD LENGTH"
    ]

    metrics_df = pd.DataFrame(results, columns=metric_names)
    out_df = pd.concat([df.reset_index(drop=True), metrics_df], axis=1)

    # Save cleaned tokens
    try:
        with open(CLEANED_CSV, "w", newline="", encoding="utf-8") as csvf:
            writer = csv.writer(csvf)
            writer.writerow(["URL", "cleaned_text"])
            for idx, row in out_df.iterrows():
                cleaned_joined = " ".join(all_cleaned_texts[idx])
                writer.writerow([row["URL"], cleaned_joined])
        print(f"Saved cleaned texts to {CLEANED_CSV}")
    except Exception as e:
        print(f"CSV save error: {e}")

    try:
        out_df.to_excel(OUTPUT_XLSX, index=False)
        print(f"Completed. Saved results to {OUTPUT_XLSX}")
        from google.colab import files
        files.download("Filled_Output.xlsx")
    except Exception as e:
        print(f"Excel save error: {e}")

if __name__ == "__main__":
    main()